# PhoBERT Fake News Detection

Dá»± Ã¡n fine-tune PhoBERT base cho tÃ¡c vá»¥ phÃ¡t hiá»‡n tin giáº£ trÃªn máº¡ng xÃ£ há»™i tiáº¿ng Viá»‡t.

## ğŸ¯ Tá»•ng quan

Dá»± Ã¡n nÃ y sá»­ dá»¥ng PhoBERT (vinai/phobert-base) Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c bÃ i Ä‘Äƒng máº¡ng xÃ£ há»™i tiáº¿ng Viá»‡t thÃ nh hai loáº¡i:
- **THáº¬T (0)**: Tin tá»©c tháº­t
- **GIáº¢ (1)**: Tin giáº£

## ğŸ“Š Dá»¯ liá»‡u

Dataset Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i tá»« format instruction sang format Ä‘Æ¡n giáº£n:
- **Train**: 14,519 máº«u (49.9% THáº¬T, 50.1% GIáº¢)
- **Validation**: 483 máº«u (83.0% THáº¬T, 17.0% GIáº¢)  
- **Test**: 486 máº«u (83.1% THáº¬T, 16.9% GIáº¢)

## ğŸš€ CÃ i Ä‘áº·t vÃ  Sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements_phobert.txt
```

### 2. Cháº¡y toÃ n bá»™ pipeline

```bash
# Cháº¡y táº¥t cáº£ cÃ¡c bÆ°á»›c
python run_phobert_pipeline.py

# Hoáº·c cháº¡y tá»«ng bÆ°á»›c riÃªng biá»‡t
python run_phobert_pipeline.py --steps data train evaluate
```

### 3. Cháº¡y tá»«ng bÆ°á»›c riÃªng láº»

#### Chuáº©n bá»‹ dá»¯ liá»‡u
```bash
python prepare_data_for_phobert.py
```

#### Training model
```bash
python train_phobert_fake_news.py
```

#### ÄÃ¡nh giÃ¡ model
```bash
python inference_phobert.py --mode evaluate
```

#### Demo tÆ°Æ¡ng tÃ¡c
```bash
python inference_phobert.py --mode demo
```

## ğŸ“ Cáº¥u trÃºc Project

```
â”œâ”€â”€ prepare_data_for_phobert.py     # Script chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u
â”œâ”€â”€ train_phobert_fake_news.py      # Script training PhoBERT
â”œâ”€â”€ inference_phobert.py            # Script inference vÃ  evaluation
â”œâ”€â”€ run_phobert_pipeline.py         # Script cháº¡y toÃ n bá»™ pipeline
â”œâ”€â”€ requirements_phobert.txt        # Dependencies cho PhoBERT
â”œâ”€â”€ README_PhoBERT.md              # HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ phobert_data/                  # Dá»¯ liá»‡u Ä‘Ã£ chuyá»ƒn Ä‘á»•i
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ phobert-fake-news-detector/    # Model Ä‘Ã£ training
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ ...
â””â”€â”€ evaluation_results/            # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
    â”œâ”€â”€ detailed_predictions.csv
    â””â”€â”€ evaluation_summary.json
```

## âš™ï¸ Cáº¥u hÃ¬nh Training

- **Model**: vinai/phobert-base
- **Max Length**: 512 tokens
- **Batch Size**: 16
- **Learning Rate**: 2e-5
- **Epochs**: 3
- **Warmup Steps**: 500
- **Weight Decay**: 0.01

## ğŸ“ˆ Sá»­ dá»¥ng Model

### Inference Ä‘Æ¡n láº»

```python
from inference_phobert import PhoBERTFakeNewsDetector

# Khá»Ÿi táº¡o detector
detector = PhoBERTFakeNewsDetector('phobert-fake-news-detector')

# Dá»± Ä‘oÃ¡n
text = "Tin tá»©c cáº§n kiá»ƒm tra..."
result = detector.predict_single(text)

print(f"Dá»± Ä‘oÃ¡n: {result['predicted_class']}")
print(f"Äá»™ tin cáº­y: {result['confidence']:.4f}")
print(f"XÃ¡c suáº¥t THáº¬T: {result['probabilities']['THáº¬T']:.4f}")
print(f"XÃ¡c suáº¥t GIáº¢: {result['probabilities']['GIáº¢']:.4f}")
```

### Inference batch

```python
texts = ["Tin 1", "Tin 2", "Tin 3"]
results = detector.predict_batch(texts)

for i, result in enumerate(results):
    print(f"Text {i+1}: {result['predicted_class']} ({result['confidence']:.4f})")
```

## ğŸ”§ TÃ¹y chá»‰nh

### Thay Ä‘á»•i cáº¥u hÃ¬nh training

Chá»‰nh sá»­a cÃ¡c tham sá»‘ trong `train_phobert_fake_news.py`:

```python
# Cáº¥u hÃ¬nh
MODEL_NAME = "vinai/phobert-base"  # CÃ³ thá»ƒ thay báº±ng phobert-large
MAX_LENGTH = 512                   # TÄƒng náº¿u cáº§n xá»­ lÃ½ text dÃ i hÆ¡n
BATCH_SIZE = 16                    # Giáº£m náº¿u thiáº¿u GPU memory
LEARNING_RATE = 2e-5               # Äiá»u chá»‰nh learning rate
NUM_EPOCHS = 3                     # TÄƒng sá»‘ epochs náº¿u cáº§n
```

### Sá»­ dá»¥ng model khÃ¡c

```python
# Thay Ä‘á»•i MODEL_NAME trong train_phobert_fake_news.py
MODEL_NAME = "vinai/phobert-large"  # Sá»­ dá»¥ng PhoBERT large
```

## ğŸ“Š Káº¿t quáº£ ÄÃ¡nh giÃ¡

Sau khi training, káº¿t quáº£ Ä‘Ã¡nh giÃ¡ sáº½ Ä‘Æ°á»£c lÆ°u trong:
- `evaluation_results/evaluation_summary.json`: Tá»•ng há»£p metrics
- `evaluation_results/detailed_predictions.csv`: Chi tiáº¿t tá»«ng prediction

CÃ¡c metrics Ä‘Æ°á»£c tÃ­nh toÃ¡n:
- **Accuracy**: Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **Precision**: Äá»™ chÃ­nh xÃ¡c theo tá»«ng class
- **Recall**: Äá»™ phá»§ theo tá»«ng class  
- **F1-score**: Äiá»ƒm F1 weighted average

## ğŸš¨ LÆ°u Ã½

1. **GPU Memory**: Training cáº§n Ã­t nháº¥t 8GB GPU memory. Giáº£m batch_size náº¿u gáº·p lá»—i OOM.

2. **Thá»i gian Training**: Vá»›i dataset ~15K máº«u, training máº¥t khoáº£ng 1-2 giá» trÃªn GPU V100.

3. **Dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng**: Val/Test set cÃ³ nhiá»u máº«u THáº¬T hÆ¡n GIáº¢, cáº§n lÆ°u Ã½ khi Ä‘Ã¡nh giÃ¡.

4. **Text dÃ i**: CÃ¡c text dÃ i hÆ¡n 512 tokens sáº½ bá»‹ cáº¯t. CÃ³ thá»ƒ tÄƒng MAX_LENGTH náº¿u cáº§n.

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push vÃ  táº¡o Pull Request

## ğŸ“„ License

MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ™ Tham kháº£o

- [PhoBERT](https://github.com/VinAIResearch/PhoBERT)
- [Transformers](https://huggingface.co/transformers/)
- [Dataset gá»‘c](https://github.com/coderkhongodo/gpt_oss)
